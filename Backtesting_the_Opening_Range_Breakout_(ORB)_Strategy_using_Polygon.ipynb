{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iswarpatel123/earnings/blob/main/Backtesting_the_Opening_Range_Breakout_(ORB)_Strategy_using_Polygon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Backtesting the Opening Range Breakout (ORB) Strategy using Polygon.io**\n",
        "\n",
        "---\n",
        "\n",
        "**Author:**  \n",
        "[**Mohamed Gabriel**](https://www.linkedin.com/in/msmgabriel/)  \n",
        "*Software Engineer at Concretum Group*\n",
        "\n",
        "---\n",
        "\n",
        "This notebook implements a backtesting framework for the Opening Range Breakout (ORB) strategy using **Python** and **Polygon.io** data. This code allows you to replicate the results in the paper [**Can Day Trading Really Be Profitable?**](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4416622) published by Concretum Group founder [**Carlo Zarratini**](https://www.linkedin.com/in/carlozarattini/).\n",
        "\n",
        "The code was created by Mohamed Gabriel to replicate the original MATLAB code that was used to generate the results in the paper.\n",
        "\n",
        "<div style=\"background-color: #f8d7da; border: 1px solid #f5c6cb; border-radius: 4px; padding: 12px; margin: 20px 0;\">\n",
        "  <p style=\"margin: 0; color: #721c24;\"><strong>‚ö†Ô∏è <a href=\"https://x.com/ConcretumR/status/1899154722071683094\" style=\"color: #721c24; text-decoration: underline;\">Important note about the results and data</a></strong> - Please read important information about data sources and potential discrepancies on our website.</p>\n",
        "</div>\n",
        "\n",
        "For detailed explanations about this approach or to contact the authors:\n",
        "\n",
        "üìß **Email:** [info@concretumgroup.com](mailto:info@concretumgroup.com)  \n",
        "üåê **Website:** [www.concretumgroup.com](https://www.concretumgroup.com)\n",
        "\n",
        "---\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "Before running the backtest, make sure you have:\n",
        "- A Polygon.io API key (free tier works for recent data)\n",
        "- Adjusted the parameters in Cell 1 to your preferences\n",
        "- Reviewed the strategy parameters in Cell 3"
      ],
      "metadata": {
        "id": "8XwzU0gig79M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmu0Crci41tf"
      },
      "source": [
        " 1. Global Parameters and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KvL-Bo941tg"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Global Parameters & Imports\n",
        "# ================================\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import matplotlib.ticker as mticker\n",
        "import pytz\n",
        "import math\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# ===== Configuration =====\n",
        "# Please set your Polygon.io API key below:\n",
        "API_KEY = \"POLYGON_API_KEY\"\n",
        "\n",
        "# Set to True if you have a paid Polygon subscription; otherwise, set to False.\n",
        "PAID_POLYGON_SUBSCRIPTION = False\n",
        "\n",
        "# Trading parameters (feel free to modify these)\n",
        "TICKER = \"TQQQ\"\n",
        "START_DATE = \"2016-01-01\"\n",
        "END_DATE = \"2025-02-21\"\n",
        "OUTPUT_FILE = f\"{TICKER}_intraday_data.csv\"\n",
        "\n",
        "utc_tz = pytz.timezone('UTC')\n",
        "nyc_tz = pytz.timezone('America/New_York')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA5fgs7b41tg"
      },
      "source": [
        "2. Function Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKF7kGKe41tg"
      },
      "outputs": [],
      "source": [
        "# ================================\n",
        "# Data Download Functions\n",
        "# ================================\n",
        "def get_polygon_data(url=None, ticker=TICKER, multiplier=1, timespan=\"minute\",\n",
        "                       from_date=START_DATE, to_date=END_DATE, adjusted=False):\n",
        "    \"\"\"Retrieve intraday aggregate data from Polygon.io.\"\"\"\n",
        "    if url is None:\n",
        "        url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker}/range/{multiplier}/{timespan}/{from_date}/{to_date}\"\n",
        "        params = {\n",
        "            \"adjusted\": \"true\" if adjusted else \"false\",\n",
        "            \"sort\": \"asc\",\n",
        "            \"limit\": 50000,\n",
        "            \"apiKey\": API_KEY\n",
        "        }\n",
        "        response = requests.get(url, params=params)\n",
        "    else:\n",
        "        if \"apiKey\" not in url:\n",
        "            url = f\"{url}&apiKey={API_KEY}\" if \"?\" in url else f\"{url}?apiKey={API_KEY}\"\n",
        "        response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return None, None\n",
        "\n",
        "    data = response.json()\n",
        "    next_url = data.get(\"next_url\")\n",
        "    if next_url and \"apiKey\" not in next_url:\n",
        "        next_url = f\"{next_url}&apiKey={API_KEY}\" if \"?\" in next_url else f\"{next_url}?apiKey={API_KEY}\"\n",
        "\n",
        "    return data.get(\"results\", []), next_url\n",
        "\n",
        "def get_daily_adjusted_data():\n",
        "    \"\"\"Fetch daily adjusted data for calculating the opening price and ATR.\n",
        "\n",
        "    Uses ATR_START_DATE = 30 days before START_DATE.\n",
        "    \"\"\"\n",
        "    ATR_START_DATE = (datetime.strptime(START_DATE, '%Y-%m-%d') - timedelta(days=30)).strftime('%Y-%m-%d')\n",
        "\n",
        "    print(\"Fetching daily adjusted data for ATR calculation...\")\n",
        "    url = f\"https://api.polygon.io/v2/aggs/ticker/{TICKER}/range/1/day/{ATR_START_DATE}/{END_DATE}\"\n",
        "    params = {\n",
        "        \"adjusted\": \"true\",\n",
        "        \"sort\": \"asc\",\n",
        "        \"limit\": 50000,\n",
        "        \"apiKey\": API_KEY\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Error getting daily data: {response.status_code}\")\n",
        "        print(response.text)\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    data = response.json().get(\"results\", [])\n",
        "    if not data:\n",
        "        print(\"No daily data found.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    df['datetime_utc'] = pd.to_datetime(df['t'], unit='ms')\n",
        "    df['datetime_et'] = df['datetime_utc'].dt.tz_localize(utc_tz).dt.tz_convert(nyc_tz)\n",
        "    df['day'] = df['datetime_et'].dt.date.astype(str)\n",
        "    df = df.rename(columns={'o': 'dOpen', 'h': 'dHigh', 'l': 'dLow', 'c': 'dClose', 'v': 'dVolume'})\n",
        "    df = calculate_atr(df, period=14)\n",
        "    df = df[df['datetime_et'] >= pd.Timestamp(START_DATE, tz=nyc_tz)].copy()\n",
        "    return df[['day', 'dOpen', 'ATR']]\n",
        "\n",
        "# ================================\n",
        "# ATR Calculation & Data Processing\n",
        "# ================================\n",
        "def calculate_atr(df, period=14):\n",
        "    \"\"\"Calculate the Average True Range (ATR) over a given period.\"\"\"\n",
        "    if 'dHigh' not in df.columns or 'dLow' not in df.columns or 'dClose' not in df.columns:\n",
        "        print(\"Missing required columns for ATR calculation\")\n",
        "        return df\n",
        "\n",
        "    df = df.copy()\n",
        "    df['prev_close'] = df['dClose'].shift(1)\n",
        "    df['HC'] = abs(df['dHigh'] - df['prev_close'])\n",
        "    df['LC'] = abs(df['prev_close'] - df['dLow'])\n",
        "    df['HL'] = abs(df['dHigh'] - df['dLow'])\n",
        "    df['TR'] = df[['HC', 'LC', 'HL']].max(axis=1)\n",
        "    df['ATR'] = df['TR'].rolling(window=period, min_periods=1).mean()\n",
        "    df['ATR'] = df['ATR'].shift(1)\n",
        "    df.drop(['prev_close', 'HC', 'LC', 'HL', 'TR'], axis=1, inplace=True)\n",
        "    return df\n",
        "\n",
        "def process_data(results):\n",
        "    \"\"\"Process raw intraday data from Polygon.io into a clean DataFrame.\"\"\"\n",
        "    if not results:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df['datetime_utc'] = pd.to_datetime(df['t'], unit='ms')\n",
        "    df['datetime_et'] = df['datetime_utc'].dt.tz_localize(utc_tz).dt.tz_convert(nyc_tz)\n",
        "    df['caldt'] = df['datetime_et'].dt.tz_localize(None)\n",
        "\n",
        "    # Filter to regular market hours (9:30-15:59 ET)\n",
        "    df = df.set_index('datetime_et')\n",
        "    market_data = df.between_time('09:30', '15:59').reset_index()\n",
        "\n",
        "    market_data['date'] = market_data['datetime_et'].dt.date\n",
        "    market_data = market_data.rename(columns={\n",
        "        'v': 'volume',\n",
        "        'vw': 'vwap',\n",
        "        'o': 'open',\n",
        "        'c': 'close',\n",
        "        'h': 'high',\n",
        "        'l': 'low',\n",
        "        't': 'timestamp_ms',\n",
        "        'n': 'transactions'\n",
        "    })\n",
        "    market_data['day'] = market_data['date'].astype(str)\n",
        "\n",
        "    return market_data\n",
        "\n",
        "def download_and_merge_data():\n",
        "    \"\"\"\n",
        "    Download intraday and daily adjusted data, merge them, and save to CSV.\n",
        "\n",
        "    If PAID_POLYGON_SUBSCRIPTION is False, simply warn or stop if START_DATE is older than 2 years.\n",
        "    \"\"\"\n",
        "    if not PAID_POLYGON_SUBSCRIPTION:\n",
        "        two_years_ago = datetime.now() - timedelta(days=730)\n",
        "        start_dt = datetime.strptime(START_DATE, '%Y-%m-%d')\n",
        "        if start_dt < two_years_ago:\n",
        "            print(\"ERROR: For free Polygon subscriptions, START_DATE must be within the past 2 years.\")\n",
        "            return None\n",
        "\n",
        "    daily_adjusted_data = get_daily_adjusted_data()\n",
        "    if daily_adjusted_data.empty:\n",
        "        print(\"Unable to get adjusted daily data. Exiting.\")\n",
        "        return None\n",
        "\n",
        "    # Download all data first, then process it all at once\n",
        "    all_raw_data = []\n",
        "    next_url = None\n",
        "    batch_count = 0\n",
        "    print(f\"Downloading intraday data for {TICKER} from {START_DATE}...\")\n",
        "\n",
        "    while True:\n",
        "        batch_count += 1\n",
        "        print(f\"Batch {batch_count}...\")\n",
        "        results, next_url = get_polygon_data(url=next_url, adjusted=False)\n",
        "        if not results:\n",
        "            print(\"No more data.\")\n",
        "            break\n",
        "\n",
        "        # Just add raw results to our collection, don't process yet\n",
        "        all_raw_data.extend(results)\n",
        "        print(f\"Batch {batch_count}: Retrieved {len(results)} records\")\n",
        "\n",
        "        if not next_url:\n",
        "            print(\"Download complete.\")\n",
        "            break\n",
        "\n",
        "        if not PAID_POLYGON_SUBSCRIPTION:\n",
        "            # Enforce a rate limit: 5 requests per minute (sleep for 12 seconds)\n",
        "            time.sleep(12)\n",
        "\n",
        "    if all_raw_data:\n",
        "        # Now process all data at once\n",
        "        print(f\"Processing {len(all_raw_data)} total records...\")\n",
        "        final_df = process_data(all_raw_data)\n",
        "\n",
        "        if not final_df.empty:\n",
        "            final_df = pd.merge(final_df, daily_adjusted_data, on='day', how='left')\n",
        "            final_df['caldt'] += pd.Timedelta(minutes=1)\n",
        "            cols = ['caldt', 'open', 'high', 'low', 'close', 'volume', 'vwap', 'transactions', 'day', 'dOpen', 'ATR']\n",
        "            available_cols = [col for col in cols if col in final_df.columns]\n",
        "            final_df[available_cols].to_csv(OUTPUT_FILE, index=False)\n",
        "            print(f\"Data saved to {OUTPUT_FILE}\")\n",
        "            print(f\"Total records: {len(final_df)}\")\n",
        "            return final_df\n",
        "        else:\n",
        "            print(\"No data after processing.\")\n",
        "            return None\n",
        "    else:\n",
        "        print(\"No data collected.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Performance Analysis & Backtesting Functions\n",
        "# ================================\n",
        "def price2return(price, n=1):\n",
        "    \"\"\"Convert a series of prices into returns.\"\"\"\n",
        "    price = np.array(price)\n",
        "    T = len(price)\n",
        "    y = np.full_like(price, np.nan, dtype=float)\n",
        "    if T > n:\n",
        "        y[n:] = price[n:] / price[:T-n] - 1\n",
        "    return y\n",
        "\n",
        "def summary_statistics(dailyReturns):\n",
        "    \"\"\"Calculate performance metrics and return a summary table.\"\"\"\n",
        "    riskFreeRate = 0\n",
        "    tradingDays = 252\n",
        "    dailyReturns = np.array(dailyReturns)\n",
        "    dailyReturns = dailyReturns[~np.isnan(dailyReturns)]\n",
        "    totalReturn = np.prod(1 + dailyReturns) - 1\n",
        "    numYears = len(dailyReturns) / tradingDays\n",
        "    CAGR = (1 + totalReturn)**(1/numYears) - 1\n",
        "    volatility = np.std(dailyReturns, ddof=0) * np.sqrt(tradingDays)\n",
        "    sharpeRatio = (np.mean(dailyReturns) - riskFreeRate/tradingDays) / np.std(dailyReturns, ddof=0) * np.sqrt(tradingDays)\n",
        "    nav = np.cumprod(1 + dailyReturns)\n",
        "    peak = np.maximum.accumulate(nav)\n",
        "    drawdown = (nav - peak) / peak\n",
        "    MDD = np.min(drawdown)\n",
        "    metrics = [\"Total Return (%)\", \"CAGR (%)\", \"Volatility (%)\", \"Sharpe Ratio\", \"Max Drawdown (%)\"]\n",
        "    values = [totalReturn*100, CAGR*100, volatility*100, sharpeRatio, MDD*100]\n",
        "    formatted_values = [f\"{v:.4f}\" if i < 3 or i == 4 else f\"{v:.6f}\" for i,v in enumerate(values)]\n",
        "    performance_table = pd.DataFrame({'Metric': metrics, 'Value': formatted_values})\n",
        "    return performance_table\n",
        "\n",
        "def monthly_performance_table(returns, dates):\n",
        "    \"\"\"Create a table of monthly returns.\"\"\"\n",
        "    returns_series = pd.Series(returns, index=pd.DatetimeIndex(dates))\n",
        "    returns_series = returns_series[~np.isnan(returns_series)]\n",
        "    df = pd.DataFrame({'return': returns_series,\n",
        "                       'year': returns_series.index.year,\n",
        "                       'month': returns_series.index.month})\n",
        "    monthly_returns = df.groupby(['year', 'month'])['return'].apply(lambda x: np.prod(1 + x) - 1).reset_index()\n",
        "    pivot_table = monthly_returns.pivot(index='year', columns='month', values='return')\n",
        "    pivot_table['Year Total'] = pivot_table.apply(lambda row: np.prod(1 + row.dropna()) - 1\n",
        "                                                   if not row.dropna().empty else np.nan, axis=1)\n",
        "    formatted_table = pivot_table.apply(lambda col: col.map(lambda x: f\"{x*100:.2f}%\" if not pd.isna(x) else \"\"))\n",
        "    month_names = {1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
        "                   7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'}\n",
        "    formatted_table = formatted_table.rename(columns=month_names)\n",
        "    return formatted_table\n",
        "\n",
        "def backtest(days, p, orb_m, target_R, risk, max_Lev, AUM_0, commission):\n",
        "    \"\"\"Perform an optimized backtest for the ORB strategy.\"\"\"\n",
        "    start_time = time.time()\n",
        "    str_df = pd.DataFrame()\n",
        "    str_df['Date'] = days\n",
        "    str_df['AUM'] = np.nan\n",
        "    str_df.loc[0, 'AUM'] = AUM_0\n",
        "    str_df['pnl_R'] = np.nan\n",
        "    or_candles = orb_m\n",
        "    day_groups = dict(tuple(p.groupby(p['day'].dt.date)))\n",
        "\n",
        "    for t in range(1, len(days)):\n",
        "        current_day = days[t].date()\n",
        "        if current_day not in day_groups:\n",
        "            str_df.loc[t, 'pnl_R'] = 0\n",
        "            str_df.loc[t, 'AUM'] = str_df.loc[t-1, 'AUM']\n",
        "            continue\n",
        "\n",
        "        day_data = day_groups[current_day]\n",
        "        if len(day_data) <= or_candles:\n",
        "            str_df.loc[t, 'pnl_R'] = 0\n",
        "            str_df.loc[t, 'AUM'] = str_df.loc[t-1, 'AUM']\n",
        "            continue\n",
        "\n",
        "        OHLC = day_data[['open', 'high', 'low', 'close']].values\n",
        "        split_adj = OHLC[0, 0] / day_data['dOpen'].iloc[0]\n",
        "        atr_raw = day_data['ATR'].iloc[0] * split_adj\n",
        "        side = np.sign(OHLC[or_candles-1, 3] - OHLC[0, 0])\n",
        "        entry = OHLC[or_candles, 0] if len(OHLC) > or_candles else np.nan\n",
        "\n",
        "\n",
        "        if side == 1:\n",
        "            stop = abs(np.min(OHLC[:or_candles, 2]) / entry - 1)\n",
        "        elif side == -1:\n",
        "            stop = abs(np.max(OHLC[:or_candles, 1]) / entry - 1)\n",
        "        else:\n",
        "            stop = np.nan\n",
        "\n",
        "        if side == 0 or math.isnan(stop) or math.isnan(entry):\n",
        "            str_df.loc[t, 'pnl_R'] = 0\n",
        "            str_df.loc[t, 'AUM'] = str_df.loc[t-1, 'AUM']\n",
        "            continue\n",
        "\n",
        "        if entry == 0 or stop == 0:\n",
        "            shares = 0\n",
        "        else:\n",
        "            shares = math.floor(min(str_df.loc[t-1, 'AUM'] * risk / (entry * stop),\n",
        "                                    max_Lev * str_df.loc[t-1, 'AUM'] / entry))\n",
        "\n",
        "        if shares == 0:\n",
        "            str_df.loc[t, 'pnl_R'] = 0\n",
        "            str_df.loc[t, 'AUM'] = str_df.loc[t-1, 'AUM']\n",
        "            continue\n",
        "\n",
        "        OHLC_post_entry = OHLC[or_candles:, :]\n",
        "\n",
        "        if side == 1:  # Long trade\n",
        "            stop_price = entry * (1 - stop)\n",
        "            target_price = entry * (1 + target_R * stop) if np.isfinite(target_R) else float('inf')\n",
        "            stop_hits = OHLC_post_entry[:, 2] <= stop_price\n",
        "            target_hits = OHLC_post_entry[:, 1] > target_price\n",
        "\n",
        "            if np.any(stop_hits) and np.any(target_hits):\n",
        "                idx_stop = np.argmax(stop_hits)\n",
        "                idx_target = np.argmax(target_hits)\n",
        "                if idx_target < idx_stop:\n",
        "                    PnL_T = max(target_price, OHLC_post_entry[idx_target, 0]) - entry\n",
        "                else:\n",
        "                    PnL_T = min(stop_price, OHLC_post_entry[idx_stop, 0]) - entry\n",
        "            elif np.any(stop_hits):\n",
        "                idx_stop = np.argmax(stop_hits)\n",
        "                PnL_T = min(stop_price, OHLC_post_entry[idx_stop, 0]) - entry\n",
        "            elif np.any(target_hits):\n",
        "                idx_target = np.argmax(target_hits)\n",
        "                PnL_T = max(target_price, OHLC_post_entry[idx_target, 0]) - entry\n",
        "            else:\n",
        "                PnL_T = OHLC_post_entry[-1, 3] - entry\n",
        "        elif side == -1:  # Short trade\n",
        "            stop_price = entry * (1 + stop)\n",
        "            target_price = entry * (1 - target_R * stop) if np.isfinite(target_R) else 0\n",
        "            stop_hits = OHLC_post_entry[:, 1] >= stop_price\n",
        "            target_hits = OHLC_post_entry[:, 2] < target_price\n",
        "\n",
        "            if np.any(stop_hits) and np.any(target_hits):\n",
        "                idx_stop = np.argmax(stop_hits)\n",
        "                idx_target = np.argmax(target_hits)\n",
        "                if idx_target < idx_stop:\n",
        "                    PnL_T = entry - min(target_price, OHLC_post_entry[idx_target, 0])\n",
        "                else:\n",
        "                    PnL_T = entry - max(stop_price, OHLC_post_entry[idx_stop, 0])\n",
        "            elif np.any(stop_hits):\n",
        "                idx_stop = np.argmax(stop_hits)\n",
        "                PnL_T = entry - max(stop_price, OHLC_post_entry[idx_stop, 0])\n",
        "            elif np.any(target_hits):\n",
        "                idx_target = np.argmax(target_hits)\n",
        "                PnL_T = entry - min(target_price, OHLC_post_entry[idx_target, 0])\n",
        "            else:\n",
        "                PnL_T = entry - OHLC_post_entry[-1, 3]\n",
        "\n",
        "        str_df.loc[t, 'AUM'] = str_df.loc[t-1, 'AUM'] + shares * PnL_T - shares * commission * 2\n",
        "        str_df.loc[t, 'pnl_R'] = (str_df.loc[t, 'AUM'] - str_df.loc[t-1, 'AUM']) / (risk * str_df.loc[t-1, 'AUM'])\n",
        "\n",
        "    end_time = time.time()\n",
        "    print(f\"******** Optimized Backtest Completed in {round(end_time - start_time, 2)} seconds! ********\")\n",
        "    print(f\"Starting AUM: ${AUM_0:,.2f}\")\n",
        "    print(f\"Final AUM: ${str_df['AUM'].iloc[-1]:,.2f}\")\n",
        "    print(f\"Total Return: {(str_df['AUM'].iloc[-1]/AUM_0 - 1)*100:.4f}%\")\n",
        "    return str_df\n",
        "\n",
        "def plot_equity_curve(str_df, AUM_0, orb_m, target_R, ticker):\n",
        "    \"\"\"Plot the equity curve with weekly resampling and highlight out-of-sample period.\"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    df_plot = str_df.copy()\n",
        "    if 'Date' in df_plot.columns:\n",
        "        df_plot = df_plot.set_index('Date')\n",
        "    try:\n",
        "        weekly_data = df_plot['AUM'].resample('W').last().dropna()\n",
        "    except Exception as e:\n",
        "        print(\"Resampling failed, using original data.\", e)\n",
        "        weekly_data = df_plot['AUM'].dropna()\n",
        "\n",
        "    p1, = ax.plot(weekly_data.index, weekly_data.values, 'r-', linewidth=2, label='Equity')\n",
        "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %y'))\n",
        "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
        "    plt.xticks(rotation=90)\n",
        "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x:,.0f}'))\n",
        "    ax.grid(True, linestyle=':')\n",
        "\n",
        "    min_val = weekly_data.min() if not weekly_data.empty else AUM_0\n",
        "    max_val = weekly_data.max() if not weekly_data.empty else AUM_0\n",
        "    ax.set_ylim([0.9 * min_val, 1.25 * max_val])\n",
        "\n",
        "    target_str = f\"Target {target_R}R\" if np.isfinite(target_R) else \"No Target\"\n",
        "    ax.set_title(f\"{orb_m}m-ORB - Stop @ OR High/Low - {target_str}\\nFull Period - Ticker = {ticker}\", fontsize=12)\n",
        "\n",
        "    # Highlight out-of-sample period starting from a specific date\n",
        "    start_date = datetime(2023, 2, 17)\n",
        "    if not weekly_data.empty and start_date >= weekly_data.index[0] and start_date <= weekly_data.index[-1]:\n",
        "        p2 = ax.axvspan(start_date, weekly_data.index[-1], alpha=0.1, color='green', label='Out-of-Sample')\n",
        "        ax.legend(handles=[p1, p2], loc='upper left')\n",
        "    else:\n",
        "        ax.legend(loc='upper left')\n",
        "\n",
        "    ax.set_yscale('log')\n",
        "    ax.yaxis.set_major_locator(mticker.LogLocator(base=10.0, subs=None))\n",
        "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'${x:,.0f}'))\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "# ================================\n",
        "# Downloading and Loading into Memory\n",
        "# ================================\n",
        "# Download & merge data\n",
        "data = download_and_merge_data()\n",
        "if data is None:\n",
        "    raise Exception(\"Data download failed.\")\n",
        "\n",
        "# Load the exported intraday data\n",
        "p = pd.read_csv(OUTPUT_FILE, parse_dates=['caldt', 'day'])\n",
        "days = pd.to_datetime(p['day'].unique())\n",
        "days = pd.DatetimeIndex(sorted(days))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycBthm9r41th"
      },
      "source": [
        "3. Backtest Parameters and Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXbAgk9741th"
      },
      "outputs": [],
      "source": [
        "# ----------------------\n",
        "# Backtest & Strategy Parameters (Edit as needed)\n",
        "# ----------------------\n",
        "orb_m       = 5             # Opening Range (minutes)\n",
        "target_R    = float('inf')  # Profit target (use inf for no target)\n",
        "commission  = 0.0005        # Commission per share\n",
        "risk        = 0.01          # Equity risk per trade (1% of AUM)\n",
        "max_Lev     = 4             # Maximum leverage\n",
        "AUM_0       = 25000         # Starting capital\n",
        "\n",
        "# ----------------------\n",
        "# Run the Backtest\n",
        "# ----------------------\n",
        "str_df = backtest(days, p, orb_m, target_R, risk, max_Lev, AUM_0, commission)\n",
        "\n",
        "# ----------------------\n",
        "# Performance Analysis\n",
        "# ----------------------\n",
        "returns = price2return(str_df['AUM'].values)\n",
        "\n",
        "display(Markdown(\"### Performance Summary\"))\n",
        "summary_stats = summary_statistics(returns)\n",
        "display(summary_stats)\n",
        "\n",
        "display(Markdown(\"### Monthly Performance\"))\n",
        "monthly_table = monthly_performance_table(returns, str_df['Date'])\n",
        "display(monthly_table)\n",
        "\n",
        "# ----------------------\n",
        "# Equity Curve Visualization\n",
        "# ----------------------\n",
        "fig, ax = plot_equity_curve(str_df, AUM_0, orb_m, target_R, TICKER)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}